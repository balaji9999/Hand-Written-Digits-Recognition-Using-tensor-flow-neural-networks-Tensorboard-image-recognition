{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(111)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(222)\n",
    "\n",
    "#from numpy.random import seed\n",
    "#seed(404)\n",
    "#import tensorflow\n",
    "#tensorflow.random_set_seed(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "from time import strftime\n",
    "import tensorboard\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_path='digit_xtrain.csv'\n",
    "x_test_path='digit_xtest.csv'\n",
    "y_train_path='digit_ytrain.csv'\n",
    "y_test_path='digit_ytest.csv'\n",
    "no_classes=10\n",
    "LOGGING_PATH=\"tensorboard_mnist_digit_logs/\"\n",
    "\n",
    "\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loding data into numpy arrays\n",
    "y_train_all=np.loadtxt(y_train_path,delimiter=',',dtype=int)\n",
    "y_test1=np.loadtxt(y_test_path,delimiter=',',dtype=int)\n",
    "x_train_all=np.loadtxt(x_train_path,delimiter=',',dtype=int)\n",
    "x_test1=np.loadtxt(x_test_path,delimiter=',',dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RE-SCALING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test = x_train_all / 255.0, x_test1 / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING LABELS INTO ONE HOT CODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.eye(no_classes)[y_test1]\n",
    "y_train_all=np.eye(no_classes)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING VALIDATION DATA SET FROM TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation data set from training dataset(10000:50000)\n",
    "x_val=x_train_all[:10000]\n",
    "x_train=x_train_all[10000:]\n",
    "y_val=y_train_all[:10000]\n",
    "y_train=y_train_all[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mange\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tensor setup\n",
    "x=tf.placeholder(dtype=tf.float32,shape=[None,784],name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.placeholder(tf.float32,shape=[None,NR_CLASSES ],name='Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs=20\n",
    "learning_rate=0.001\n",
    "total_inputs=784\n",
    "n_hidden_1=512\n",
    "n_hidden_2=64\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_scope is used to group all the layer elements(w,b,activation) in graph\n",
    "with tf.name_scope('hidden_1'):\n",
    "    initial_w1=tf.truncated_normal(shape=[784,512],stddev=0.1,seed=42)\n",
    "    w1=tf.Variable(initial_value=initial_w1,name='w1')\n",
    "    initial_b1=tf.constant(value=0.0,shape=[512])\n",
    "    b1=tf.Variable(initial_value=initial_b1,name='b1')\n",
    "    layer1_in=tf.matmul(x,w1)+b1\n",
    "    layer1_out=tf.nn.relu(layer1_in)\n",
    "    tf.summary.histogram('weights',w1)\n",
    "    tf.summary.histogram('biases',b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('hidden_2'):\n",
    "\n",
    "    initial_w2=tf.truncated_normal(shape=[512,64],stddev=0.1,seed=42)\n",
    "    w2=tf.Variable(initial_value=initial_w2,name='w2')\n",
    "    initial_b2=tf.constant(value=0.0,shape=[64])\n",
    "    b2=tf.Variable(initial_value=initial_b2,name='b2')\n",
    "    layer2_in=tf.matmul(layer1_out,w2)+b2\n",
    "    layer2_out=tf.nn.relu(layer2_in)\n",
    "    \n",
    "    tf.summary.histogram('weights',w2)\n",
    "    tf.summary.histogram('biases',b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('output_layer'):\n",
    "\n",
    "    initial_w3=tf.truncated_normal(shape=[64,10],stddev=0.1,seed=42)\n",
    "    w3=tf.Variable(initial_value=initial_w3,name='w3')\n",
    "    initial_b3=tf.constant(value=0.0,shape=[10])\n",
    "    b3=tf.Variable(initial_value=initial_b3,name='b3')\n",
    "    layer3_in=tf.matmul(layer2_out,w3)+b3\n",
    "    output=tf.nn.softmax(layer3_in)\n",
    "    \n",
    "    tf.summary.histogram('weights',w3)\n",
    "    tf.summary.histogram('biases',b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#instead of defining each hidden layer individually we will define function so that we can use it when ever necessary\n",
    "\n",
    "def setup_layer(input,weight_dim,bias_dim,name):\n",
    "    with tf.name_scope(name):\n",
    "        initial_w=tf.truncated_normal(shape=weight_dim,stddev=0.1,seed=42)\n",
    "        w=tf.Variable(initial_value=initial_w,name='W')\n",
    "        initial_b=tf.constant(value=0.0,shape=bias_dim)\n",
    "        b=tf.Variable(initial_value=initial_b,name='B')\n",
    "        layer_in=tf.matmul(input,w)+b\n",
    "        if name=='output':\n",
    "             layer_out=tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "             layer_out=tf.nn.relu(layer_in)\n",
    "            \n",
    "        tf.summary.histogram('weights',w)\n",
    "        tf.summary.histogram('biases',b)\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_1=setup_layer(x,weight_dim=[TOTAL_INPUTS,n_hidden_1],\n",
    "                   bias_dim=[n_hidden_1],name='layer_1')\n",
    "\n",
    "layer_2=setup_layer(layer_1,weight_dim=[n_hidden_1,n_hidden_2],\n",
    "                   bias_dim=[n_hidden_2],name='layer_2')\n",
    " \n",
    "output=setup_layer(layer_2,weight_dim=[n_hidden_2,NR_CLASSES],\n",
    "                   bias_dim=[NR_CLASSES],name='output')\n",
    "\n",
    "model_name=f'{n_hidden_1}-{n_hidden_2} LR {learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=f'{n_hidden_1}-{n_hidden_2} LR {learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORBOARD SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESSFULLY CREATED DIRECTORIES\n"
     ]
    }
   ],
   "source": [
    "##TENSOR BOARD FLODER\n",
    "folder_name= f'{model_name}_at_{strftime(\"%H_%M\")}'\n",
    "directory=os.path.join(LOGGING_PATH,folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('SUCESSFULLY CREATED DIRECTORIES')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS,OPTIMISATION & METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DEFINING LOSS FUNCTION\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=output))\n",
    "\n",
    "#DEFINING OPTIMIZER\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "train_step=optimizer.minimize(loss)\n",
    "\n",
    "#ACCURACY METRICS\n",
    "model_prediction=tf.argmax(output,axis=1,name='prediction')\n",
    "correct_pred=tf.equal(model_prediction,tf.argmax(y,axis=1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction=tf.argmax(output,axis=1,name='prediction')\n",
    "    correct_pred=tf.equal(model_prediction,tf.argmax(y,axis=1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#summary for accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.scalar('Loss',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK INPUT IMAGES IN TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image=tf.reshape(x,[-1,28,28,1])\n",
    "    tf.summary.image('image_input',x_image,max_outputs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETup FILEWRITER and MERGE Summaries\n",
    "merged_summary=tf.summary.merge_all()\n",
    "\n",
    "###TRAINING WRITER\n",
    "train_writer=tf.summary.FileWriter(directory+'/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "###VALIDATION WRITER\n",
    "validation_writer=tf.summary.FileWriter(directory+'/validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ###initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch=1000\n",
    "num_examples=y_train.shape[0]\n",
    "nr_iterations=int(num_examples/size_of_batch)\n",
    "index_in_epoch=0\n",
    "\n",
    "def next_batch(batch_size,data,labels):\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        start=0\n",
    "        index_in_epoch=batch_size\n",
    "    \n",
    "    end=index_in_epoch\n",
    "    \n",
    "    return data[start:end] ,labels[start:end]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t|Training Accuracy =0.7760000228881836\n",
      "Epoch 1 \t|Training Accuracy =0.7850000262260437\n",
      "Epoch 2 \t|Training Accuracy =0.8769999742507935\n",
      "Epoch 3 \t|Training Accuracy =0.9629999995231628\n",
      "Epoch 4 \t|Training Accuracy =0.9739999771118164\n",
      "Epoch 5 \t|Training Accuracy =0.9800000190734863\n",
      "Epoch 6 \t|Training Accuracy =0.984000027179718\n",
      "Epoch 7 \t|Training Accuracy =0.9879999756813049\n",
      "Epoch 8 \t|Training Accuracy =0.9879999756813049\n",
      "Epoch 9 \t|Training Accuracy =0.9879999756813049\n",
      "Epoch 10 \t|Training Accuracy =0.9860000014305115\n",
      "Epoch 11 \t|Training Accuracy =0.9869999885559082\n",
      "Epoch 12 \t|Training Accuracy =0.9890000224113464\n",
      "Epoch 13 \t|Training Accuracy =0.9900000095367432\n",
      "Epoch 14 \t|Training Accuracy =0.9900000095367432\n",
      "Epoch 15 \t|Training Accuracy =0.9900000095367432\n",
      "Epoch 16 \t|Training Accuracy =0.9909999966621399\n",
      "Epoch 17 \t|Training Accuracy =0.9909999966621399\n",
      "Epoch 18 \t|Training Accuracy =0.9909999966621399\n",
      "Epoch 19 \t|Training Accuracy =0.9909999966621399\n",
      "completed>>>>\n"
     ]
    }
   ],
   "source": [
    "# TRAINING_loop\n",
    "    \n",
    "for epoch in range (nr_epochs):\n",
    "    \n",
    "    ####TRAINING DATA SET#####\n",
    "    for i in range (nr_iterations):\n",
    "        batch_x,batch_y=next_batch(size_of_batch,data=x_train,labels=y_train)\n",
    "        feed_dictionary = {x:batch_x, y:batch_y}\n",
    "        sess.run(train_step,feed_dict=feed_dictionary)\n",
    "    s,batch_accuracy=sess.run(fetches=[merged_summary,accuracy],feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(s,epoch)\n",
    "    print(f'Epoch {epoch} \\t|Training Accuracy ={batch_accuracy}')\n",
    "    \n",
    "    ########VALIDATION DATA SET######\n",
    "    summary=sess.run(fetches=merged_summary,feed_dict={x:x_val,y:y_val})\n",
    "    validation_writer.add_summary(summary,epoch)\n",
    "    \n",
    "print(\"completed>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SAVING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs={'accuracy_calc/prediction':model_prediction}\n",
    "inputs={'X':x}\n",
    "tf.saved_model.simple_save(sess,'SavedModel1993',inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x1A33A248AC8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from PIL import Image\n",
    "img=Image.open('test_img.png')\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw=img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nMVSwRGEIAxcHP+WcLZgBch15UtTynVgBcylE0vwKsg9HAhw6sNx5vYTwpIlGzCCY1Qn3G0kUZYabYjAAPxuJTGs90hr67D4vOcGgOVUVwKWLbgp7ojKPrZgr1kpITnWftWkzo8Sd82OFQDAE3Y8kB36Ib9Dycm5pWggyBIXikAcPL1adT8WZDpv1peRX8Tpmr9/sNvIL4ZNnNSjtr9VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1A33A45E148>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array=np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to flaten the array data\n",
    "test_img=img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=sess.run(fetches=tf.argmax(output,axis=1),feed_dict={x:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEAN SQUARE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "prediction1=sess.run(fetches=tf.argmax(output,axis=1),feed_dict={x:x_test})\n",
    "\n",
    "MSE=mean_squared_error(y_test1,prediction1) \n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=sess.run(fetches=accuracy,feed_dict={x:x_test,y:y_test})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 97.42%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RESET FOR THE NEXT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_writer.close() ####FOR TRAINING DATA SET\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "validation_writer.close()  #### FOR VALIDATION DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
